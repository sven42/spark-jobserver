# Template for Spark Job Server Docker config
# You can easily override the spark master through SPARK_MASTER env variable
#
# Spark Cluster / Job Server configuration
spark {
  # spark.master will be passed to each job's JobContext
  # local[...], yarn, mesos://... or spark://...
  master = "local[4]"
  master = ${?SPARK_MASTER}

  # client or cluster deployment
  submit.deployMode = "client"

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 4

  jobserver {
    port = 8090
    jobdao = spark.jobserver.io.JobSqlDAO

    context-per-jvm = true

    short-timeout = 9 s

    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.driver.PostgresDriver

      # JDBC driver, full classpath
      jdbc-driver = org.postgresql.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = "/var/lib/spark-jobserver/data"

      jdbc {
        url = "jdbc:postgresql://localhost/sparkjobserver"
        user = "sparkjobserver"
        password = <password>
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
  }

  # predefined Spark contexts
  contexts {
    py-context {
      num-cpu-cores = 1           # Number of cores to allocate.  Required.
      memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
      context-factory = spark.jobserver.python.PythonSessionContextFactory
    }
  #   my-low-latency-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #   }
  #   # define additional contexts here
  }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 2           # Number of cores to allocate.  Required.
    memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.

    # in case spark distribution should be accessed from HDFS (as opposed to being installed on every mesos slave)
    # spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]

    # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
    # such as hadoop connection settings that don't use the "spark." prefix
    passthrough {
      #es.nodes = "192.1.1.1"
    }

    python {
      paths = [
        # TODO
        /opt/spark-2.3.1/python,
        "/opt/spark-jobserver/job-server-extras/job-server-python/spark_jobserver_python-0.8.1-SNAPSHOT-py3.6.egg"
      ]

      # The default value in application.conf is "python"
      executable = "python3.6"
    }
  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  home = "/opt/spark-2.3.1"
}

flyway.locations="db/postgresql/migration"
# Note that you can use this file to define settings not only for job server,
# but for your Spark jobs as well.  Spark job configuration merges with this configuration file as defaults.
